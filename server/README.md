# Document Processing Server

A FastAPI server for uploading, parsing, editing, summarizing, and querying documents using LlamaParse, OpenAI, and Pinecone. Features a completely generic implementation that works with any business domain, file types, and data content.

## üöÄ **Features**

- **Document upload and storage** with support for PDF, DOCX, TXT, XLSX, XLS, CSV formats
- **PDF parsing with LlamaParse** for advanced document understanding
- **Document editing and summarization** with AI-powered content generation
- **Vector search with Pinecone** for semantic document retrieval
- **Hybrid search** combining dense vectors and sparse BM25 encoding
- **Excel/CSV data analysis** with pandas agent for spreadsheet queries
- **Generic implementation** - works with any business domain or data type
- **Multiple storage modes** - local filesystem, Google Cloud Storage, or auto-detection
- **Comprehensive testing** - organized test package with unit, integration, and functional tests

## üèóÔ∏è **Architecture Overview**

### **Generic Implementation**
The system is designed to be completely generic with no hardcoded business references:
- ‚úÖ **Configurable Pinecone indexes** via environment variables
- ‚úÖ **Generic file handling** for any file type
- ‚úÖ **Generic search functionality** for any data content
- ‚úÖ **Generic prompt templates** for any domain
- ‚úÖ **Environment variable configuration** for all settings

### **Storage Modes**
- **`local`** - Uses local filesystem (development, testing)
- **`cloud`** - Uses Google Cloud Storage (production, Cloud Run)  
- **`auto`** - Auto-detects based on environment (default)

### **Test Package Structure**
```
test/
‚îú‚îÄ‚îÄ unit/                           # Unit tests for individual components
‚îÇ   ‚îú‚îÄ‚îÄ test_utils.py              # Tests for utility functions (39 tests)
‚îÇ   ‚îî‚îÄ‚îÄ test_models.py             # Tests for Pydantic models (39 tests)
‚îú‚îÄ‚îÄ integration/                    # Integration tests for API endpoints
‚îÇ   ‚îú‚îÄ‚îÄ test_api_endpoints.py      # Tests for API endpoints (44 tests)
‚îÇ   ‚îî‚îÄ‚îÄ test_hybrid_search_integration.py  # Legacy hybrid search tests
‚îú‚îÄ‚îÄ functional/                     # Functional tests for complete features
‚îÇ   ‚îî‚îÄ‚îÄ test_generic_implementation.py  # Tests for generic implementation (6 tests)
‚îú‚îÄ‚îÄ conftest.py                    # Shared pytest fixtures
‚îú‚îÄ‚îÄ utils.py                       # Test utilities and helpers
‚îú‚îÄ‚îÄ pytest.ini                    # Pytest configuration
‚îú‚îÄ‚îÄ run_all_tests.py              # Comprehensive test runner
‚îî‚îÄ‚îÄ README.md                     # Comprehensive documentation
```

## ‚öôÔ∏è **Environment Setup**

Before running the server, you need to create a `.env` file with the following credentials:

```bash
OPENAI_API_KEY=your_openai_api_key
LLAMA_CLOUD_API_KEY=your_llama_cloud_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment

# Optional: Custom Pinecone configuration
PINECONE_INDEX_NAME=document-vector-store-hybrid
PINECONE_NAMESPACE=document-namespace
```

## üöÄ **Quick Start**

### **Option 1: Docker Compose (Recommended)**

1. **Create environment file:**
```bash
cp env-template.txt .env
# Edit .env with your actual API keys
```

2. **Run with Docker Compose:**
```bash
docker compose up -d --build
```

The server will be accessible at: http://localhost:8004

### **Option 2: Local Development**

```bash
# Install dependencies
pip install -r requirements.txt

# Start with local storage and auto-reload
python start_server.py --storage local --reload

# Or using uvicorn directly
STORAGE_MODE=local uvicorn app:app --reload --host 0.0.0.0 --port 8004
```

### **Option 3: Cloud Storage Testing**

```bash
# Test cloud storage locally (requires authentication)
GOOGLE_CLOUD_PROJECT=your-project-id python start_server.py --storage cloud
```

## üìã **API Endpoints**

### **File Management**
- `POST /uploadfile/` - Upload a document with automatic file type detection
- `GET /listfiles/{directory}` - List files in a directory
- `DELETE /deletefile/{directory}/{filename}` - Delete a file

### **Document Processing**
- `GET /parsefile/{filename}` - Parse an uploaded file to markdown
- `POST /savecontent/{filename}` - Save edited content
- `POST /saveandingst/{filename}` - Save content and ingest to search indexes
- `POST /ingestdocuments/{filename}` - Ingest a document into Pinecone and BM25 indexes

### **Search & Query**
- `POST /hybridsearch/` - Perform intelligent hybrid search across documents and data
- `POST /querypandas/` - Direct Excel/CSV data analysis with AI
- `GET /sourcedocuments/` - Get list of available source documents

### **AI Content Generation**
- `GET /summarizecontent/{filename}` - Generate a summary for a file
- `GET /generatequestions/{filename}` - Generate questions from a file
- `GET /generatefaq/{filename}` - Generate FAQ from a file

## üîÑ **Document Processing Flow**

1. **Upload** a document using `/uploadfile/`
2. **Parse** the document to markdown with `/parsefile/{filename}`
3. **Edit** the content if needed with `/savecontent/{filename}`
4. **Summarize** the document with `/summarizecontent/{filename}`
5. **Ingest** the document with `/ingestdocuments/{filename}`
6. **Query** the document with `/hybridsearch/` or `/querypandas/`

## üß™ **Testing**

### **Running Tests**

```bash
# Run all tests
python test/run_all_tests.py

# Run specific categories
python test/run_all_tests.py --unit
python test/run_all_tests.py --integration
python test/run_all_tests.py --functional

# Run with coverage
python test/run_all_tests.py --coverage

# Run specific test files
python test/run_all_tests.py test/unit/test_utils.py
```

### **Test Coverage**

- **Unit Tests**: ‚úÖ 39/39 passing
  - File utility tests
  - Model validation tests
  - Configuration tests
  - Meaningful result detection tests

- **Functional Tests**: ‚úÖ 6/6 passing
  - Generic configuration tests
  - Generic search functionality tests
  - Generic file handling tests
  - Generic prompt template tests
  - Environment variable configuration tests
  - No hardcoded data reference tests

- **Integration Tests**: ‚úÖ Organized and structured
  - API endpoint tests
  - Error handling tests
  - Response validation tests

## ‚òÅÔ∏è **Google Cloud Run Deployment**

**‚ö†Ô∏è Important: File Storage in Cloud Run**

Cloud Run containers have ephemeral storage, meaning files are lost when containers restart. This application automatically uses **Google Cloud Storage** for persistent file storage when deployed to Cloud Run.

The following directories are automatically mapped to Cloud Storage buckets:
- `uploaded_files` ‚Üí `{project-id}-uploaded-files`
- `parsed_files` ‚Üí `{project-id}-parsed-files` 
- `bm25_indexes` ‚Üí `{project-id}-bm25-indexes`
- `generated_questions` ‚Üí `{project-id}-generated-questions`

### **Quick Deployment (Automated)**

Use the automated deployment script (creates buckets automatically):

```bash
# Basic deployment
python deploy_to_cloudrun.py --project-id YOUR_PROJECT_ID --region us-central1

# With Secret Manager (production)
python deploy_to_cloudrun.py --project-id YOUR_PROJECT_ID --region us-central1 --use-secrets

# View all options
python deploy_to_cloudrun.py --help
```

### **Manual Deployment**

Follow these steps to manually deploy to Google Cloud Run:

#### 1. Prerequisites
```bash
# Login to GCP
gcloud auth login

# Set your project
gcloud config set project YOUR_PROJECT_ID

# Configure Docker authentication
gcloud auth configure-docker us-central1-docker.pkg.dev
```

#### 2. Enable Required APIs
```bash
gcloud services enable artifactregistry.googleapis.com
gcloud services enable run.googleapis.com
gcloud services enable cloudbuild.googleapis.com
```

#### 3. Create Artifact Registry Repository
```bash
gcloud artifacts repositories create document-processing \
    --repository-format=docker \
    --location=us-central1 \
    --description="Docker repository for document processing service"
```

#### 4. Build and Push Docker Image
```bash
# Build for Cloud Run (linux/amd64)
docker build --platform linux/amd64 \
    -t us-central1-docker.pkg.dev/YOUR_PROJECT_ID/document-processing/document-processing-server:latest .

# Push to Artifact Registry
docker push us-central1-docker.pkg.dev/YOUR_PROJECT_ID/document-processing/document-processing-server:latest
```

#### 5. Deploy to Cloud Run
```bash
# Source environment variables
source .env

# Deploy service
gcloud run deploy document-processing-service \
    --image us-central1-docker.pkg.dev/YOUR_PROJECT_ID/document-processing/document-processing-server:latest \
    --region us-central1 \
    --platform managed \
    --allow-unauthenticated \
    --set-env-vars OPENAI_API_KEY=$OPENAI_API_KEY,LLAMA_CLOUD_API_KEY=$LLAMA_CLOUD_API_KEY,PINECONE_API_KEY=$PINECONE_API_KEY,PINECONE_ENVIRONMENT=$PINECONE_ENVIRONMENT \
    --port 8004 \
    --memory 2Gi \
    --cpu 1
```

#### 6. Get Service URL
```bash
gcloud run services describe document-processing-service --region=us-central1 --format='value(status.url)'
```

### **Environment Variables**

For production deployments, consider using Google Secret Manager:

```bash
# Create secrets
echo -n "your_openai_key" | gcloud secrets create openai-api-key --data-file=-
echo -n "your_llama_key" | gcloud secrets create llama-cloud-api-key --data-file=-
echo -n "your_pinecone_key" | gcloud secrets create pinecone-api-key --data-file=-
echo -n "your_pinecone_env" | gcloud secrets create pinecone-environment --data-file=-

# Deploy with secrets
gcloud run deploy document-processing-service \
    --image us-central1-docker.pkg.dev/YOUR_PROJECT_ID/document-processing/document-processing-server:latest \
    --region us-central1 \
    --set-secrets OPENAI_API_KEY=openai-api-key:latest,LLAMA_CLOUD_API_KEY=llama-cloud-api-key:latest,PINECONE_API_KEY=pinecone-api-key:latest,PINECONE_ENVIRONMENT=pinecone-environment:latest
```

## üíæ **Storage Configuration**

The server supports multiple storage modes that can be configured based on your deployment needs:

### **Storage Modes**

| Mode | Description | Use Case | Storage Location |
|------|-------------|----------|------------------|
| `local` | Uses local filesystem | Development, testing | Local directories |
| `cloud` | Uses Google Cloud Storage | Production, Cloud Run | GCS buckets |
| `auto` | Auto-detects based on environment | Default (recommended) | Automatic selection |

### **Configuration Methods**

#### **1. Startup Script (Recommended)**
```bash
# Local development with auto-reload
python start_server.py --storage local --reload

# Cloud storage testing
python start_server.py --storage cloud

# Auto-detection (default)
python start_server.py --storage auto
```

#### **2. Environment Variables**
```bash
# Set environment variable
export STORAGE_MODE=local
uvicorn app:app --host 0.0.0.0 --port 8004

# Inline usage
STORAGE_MODE=local uvicorn app:app --reload
```

#### **3. Docker Compose**
```bash
# Local storage (default)
STORAGE_MODE=local docker compose up

# Cloud storage
STORAGE_MODE=cloud GOOGLE_CLOUD_PROJECT=your-project docker compose up
```

### **Environment Detection**

The system automatically detects the appropriate storage mode using this priority order:

1. **`STORAGE_MODE`** environment variable (highest priority)
2. **Legacy variables**: `FORCE_LOCAL_STORAGE`, `FORCE_CLOUD_STORAGE`
3. **Cloud Run detection**: `K_SERVICE` environment variable (set by Cloud Run)
4. **Explicit cloud config**: `USE_CLOUD_STORAGE=true`
5. **Google Cloud project**: `GOOGLE_CLOUD_PROJECT` or `GCP_PROJECT` variables
6. **Default**: Local storage

## üîß **Local Development**

### **Option 1: Using the Startup Script (Recommended)**
```bash
# Local storage with auto-reload
python start_server.py --storage local --reload

# Cloud storage for testing
GOOGLE_CLOUD_PROJECT=your-project-id python start_server.py --storage cloud
```

### **Option 2: Using Environment Variables**
```bash
# Local storage
STORAGE_MODE=local uvicorn app:app --reload --host 0.0.0.0 --port 8004

# Cloud storage
STORAGE_MODE=cloud GOOGLE_CLOUD_PROJECT=your-project-id uvicorn app:app --host 0.0.0.0 --port 8004
```

### **Option 3: Using Docker Compose**
```bash
# Local storage (default)
STORAGE_MODE=local docker compose up

# Cloud storage
STORAGE_MODE=cloud GOOGLE_CLOUD_PROJECT=your-project-id docker compose up
```

### **Option 4: Direct Docker Build**
```bash
cd server
docker build -t document-processing-server .
docker run -d -p 8004:8004 --name doc-server --env-file .env \
    -e STORAGE_MODE=local \
    -v $(pwd)/uploaded_files:/app/uploaded_files \
    -v $(pwd)/parsed_files:/app/parsed_files \
    -v $(pwd)/bm25_indexes:/app/bm25_indexes \
    -v $(pwd)/generated_questions:/app/generated_questions \
    document-processing-server
```

## üéØ **Generic Implementation Features**

### **‚úÖ No Hardcoded Business References**
- No references to "biz", "bricks", "smith", "family", etc. in active code
- All business-specific terms removed from configuration
- Generic naming conventions throughout

### **‚úÖ Environment Variable Configuration**
- `PINECONE_INDEX_NAME`: Configurable index name
- `PINECONE_NAMESPACE`: Configurable namespace
- `PINECONE_API_KEY`: Standard API key configuration
- `OPENAI_API_KEY`: Standard API key configuration

### **‚úÖ Generic File Processing**
- Works with any file type (PDF, DOCX, TXT, XLSX, XLS, CSV)
- No hardcoded file dependencies
- Generic file type detection

### **‚úÖ Generic Search Functionality**
- Accepts any query without business-specific logic
- Works with any uploaded documents
- No hardcoded search patterns

### **‚úÖ Generic Prompt Templates**
- No business-specific language in prompts
- Works with any domain or data type
- Generic instructions for AI responses

## üöÄ **Usage Examples**

### **Default Configuration**
```bash
# Uses default generic names
PINECONE_INDEX_NAME=document-vector-store-hybrid
PINECONE_NAMESPACE=document-namespace
```

### **Custom Configuration**
```bash
# Custom configuration for any business
export PINECONE_INDEX_NAME=my-company-vector-store
export PINECONE_NAMESPACE=my-company-namespace
```

### **Different Environments**
```bash
# Development
PINECONE_INDEX_NAME=dev-vector-store
PINECONE_NAMESPACE=dev-namespace

# Production
PINECONE_INDEX_NAME=prod-vector-store
PINECONE_NAMESPACE=prod-namespace

# Testing
PINECONE_INDEX_NAME=test-vector-store
PINECONE_NAMESPACE=test-namespace
```

## üêõ **Troubleshooting**

### **Common Issues**

1. **Permission Denied Errors**
   - Ensure you're authenticated: `gcloud auth login`
   - Check project access: `gcloud projects get-iam-policy YOUR_PROJECT_ID`

2. **Image Architecture Issues**
   - Always build with `--platform linux/amd64` for Cloud Run

3. **Port Configuration**
   - Cloud Run expects apps to listen on the PORT environment variable
   - Our Dockerfile handles this automatically

4. **Environment Variables**
   - Ensure .env file exists and contains all required keys
   - Use Secret Manager for production deployments

### **Monitoring and Logs**
```bash
# View service logs
gcloud run services logs read document-processing-service --region=us-central1

# Monitor service
gcloud run services describe document-processing-service --region=us-central1
```

## üìö **API Documentation**

Once deployed, access the interactive API documentation at:
- Local: http://localhost:8004/docs
- Cloud Run: https://YOUR_SERVICE_URL/docs

## üéâ **Summary**

The Document Processing Server is now **completely generic** and can be used for:

- ‚úÖ **Any business domain** (healthcare, finance, education, etc.)
- ‚úÖ **Any file types** (documents, spreadsheets, presentations, etc.)
- ‚úÖ **Any data content** (reports, manuals, research papers, etc.)
- ‚úÖ **Any deployment environment** (dev, staging, production)
- ‚úÖ **Any Pinecone configuration** (custom indexes, namespaces)

The implementation is **production-ready** and **enterprise-grade** with:
- ‚úÖ **Comprehensive testing** (89+ tests organized by category)
- ‚úÖ **Multiple storage modes** (local, cloud, auto-detection)
- ‚úÖ **Generic implementation** (no hardcoded dependencies)
- ‚úÖ **Professional documentation** (clear usage instructions)
- ‚úÖ **CI/CD ready** (modular test execution) 